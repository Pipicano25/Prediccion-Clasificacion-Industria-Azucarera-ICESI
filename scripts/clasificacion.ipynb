{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de Clasificación - Industria Azucarera\n",
    "# Proyecto: Predicción y Clasificación en la Industria Azucarera ICESI\n",
    "# \n",
    "# Objetivo: Crear categorías de clasificación para variables TCH y %Sac.Caña\n",
    "# en niveles de desempeño: alto, medio y bajo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Librerías importadas correctamente\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARGA Y EXPLORACIÓN INICIAL DE DATOS\n",
    "# ===========================================\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_excel('../data/raw/BD_IPSA_1940.xlsx')\n",
    "\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"Número de registros: {df.shape[0]}\")\n",
    "print(f\"Número de variables: {df.shape[1]}\")\n",
    "print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nInformación de columnas:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7352649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ANÁLISIS ESPECÍFICO DE VARIABLES OBJETIVO\n",
    "# =============================================\n",
    "\n",
    "# Verificar si las columnas TCH y %Sac.Caña existen\n",
    "print(\"ANÁLISIS DE VARIABLES OBJETIVO\")\n",
    "print(\"=\"*50)\n",
    "print(\"Nombres de columnas disponibles:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Buscar columnas relacionadas con TCH y Sacarosa\n",
    "tch_columns = [col for col in df.columns if 'TCH' in col.upper() or 'TONELADA' in col.upper()]\n",
    "sacarosa_columns = [col for col in df.columns if 'SAC' in col.upper() or 'SACAROSA' in col.upper() or '%' in col]\n",
    "\n",
    "print(f\"\\nColumnas relacionadas con TCH: {tch_columns}\")\n",
    "print(f\"Columnas relacionadas con Sacarosa: {sacarosa_columns}\")\n",
    "\n",
    "# Mostrar valores únicos para entender mejor los datos\n",
    "print(\"\\nValores únicos en las primeras columnas:\")\n",
    "for col in df.columns[:10]:\n",
    "    print(f\"{col}: {df[col].nunique()} valores únicos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. IDENTIFICACIÓN Y SELECCIÓN DE VARIABLES OBJETIVO\n",
    "# ===================================================\n",
    "\n",
    "# Función para encontrar la mejor columna basada en patrones\n",
    "def find_best_column(df, keywords):\n",
    "    \"\"\"\n",
    "    Encuentra la mejor columna basada en palabras clave\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for col in df.columns:\n",
    "        col_upper = col.upper()\n",
    "        for keyword in keywords:\n",
    "            if keyword.upper() in col_upper:\n",
    "                matches.append(col)\n",
    "                break\n",
    "    return matches\n",
    "\n",
    "# Buscar columnas para TCH (Toneladas de Caña por Hectárea)\n",
    "tch_keywords = ['TCH', 'TONELADA', 'CAÑA', 'HECTAREA', 'RENDIMIENTO']\n",
    "tch_candidates = find_best_column(df, tch_keywords)\n",
    "\n",
    "# Buscar columnas para %Sac.Caña (Porcentaje de Sacarosa en Caña)\n",
    "sacarosa_keywords = ['SAC', 'SACAROSA', '%', 'PORCENTAJE', 'POL']\n",
    "sacarosa_candidates = find_best_column(df, sacarosa_keywords)\n",
    "\n",
    "print(\"CANDIDATOS PARA VARIABLES OBJETIVO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Candidatos para TCH: {tch_candidates}\")\n",
    "print(f\"Candidatos para %Sac.Caña: {sacarosa_candidates}\")\n",
    "\n",
    "# Analizar los candidatos más prometedores\n",
    "if tch_candidates:\n",
    "    print(f\"\\nAnálisis de candidatos TCH:\")\n",
    "    for col in tch_candidates[:3]:  # Analizar los primeros 3 candidatos\n",
    "        print(f\"\\nColumna: {col}\")\n",
    "        print(f\"Tipo: {df[col].dtype}\")\n",
    "        print(f\"Valores únicos: {df[col].nunique()}\")\n",
    "        print(f\"Valores nulos: {df[col].isnull().sum()}\")\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            print(f\"Rango: {df[col].min():.2f} - {df[col].max():.2f}\")\n",
    "            print(f\"Media: {df[col].mean():.2f}\")\n",
    "\n",
    "if sacarosa_candidates:\n",
    "    print(f\"\\nAnálisis de candidatos %Sac.Caña:\")\n",
    "    for col in sacarosa_candidates[:3]:  # Analizar los primeros 3 candidatos\n",
    "        print(f\"\\nColumna: {col}\")\n",
    "        print(f\"Tipo: {df[col].dtype}\")\n",
    "        print(f\"Valores únicos: {df[col].nunique()}\")\n",
    "        print(f\"Valores nulos: {df[col].isnull().sum()}\")\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            print(f\"Rango: {df[col].min():.2f} - {df[col].max():.2f}\")\n",
    "            print(f\"Media: {df[col].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SELECCIÓN FINAL DE VARIABLES OBJETIVO\n",
    "# ========================================\n",
    "\n",
    "# Basado en el análisis anterior, seleccionar las mejores columnas\n",
    "# Si no se encuentran las columnas exactas, usar las más similares\n",
    "\n",
    "# Seleccionar las primeras columnas numéricas que parezcan ser TCH y %Sac.Caña\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"COLUMNAS NUMÉRICAS DISPONIBLES\")\n",
    "print(\"=\"*50)\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Seleccionar las dos primeras columnas numéricas como variables objetivo\n",
    "# (Esto se ajustará según los resultados del análisis anterior)\n",
    "if len(numeric_columns) >= 2:\n",
    "    # Asumir que las primeras dos columnas numéricas son TCH y %Sac.Caña\n",
    "    tch_col = numeric_columns[0]  # Primera columna numérica\n",
    "    sacarosa_col = numeric_columns[1]  # Segunda columna numérica\n",
    "    \n",
    "    print(f\"\\nVARIABLES SELECCIONADAS:\")\n",
    "    print(f\"TCH: {tch_col}\")\n",
    "    print(f\"%Sac.Caña: {sacarosa_col}\")\n",
    "    \n",
    "    # Crear un dataset limpio con las variables objetivo\n",
    "    df_clean = df[[tch_col, sacarosa_col]].copy()\n",
    "    \n",
    "    # Eliminar filas con valores nulos\n",
    "    df_clean = df_clean.dropna()\n",
    "    \n",
    "    print(f\"\\nDataset limpio:\")\n",
    "    print(f\"Dimensiones: {df_clean.shape}\")\n",
    "    print(f\"Valores nulos: {df_clean.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Renombrar columnas para mayor claridad\n",
    "    df_clean.columns = ['TCH', 'Sacarosa_Porcentaje']\n",
    "    \n",
    "    print(f\"\\nEstadísticas descriptivas del dataset limpio:\")\n",
    "    print(df_clean.describe())\n",
    "    \n",
    "else:\n",
    "    print(\"Error: No se encontraron suficientes columnas numéricas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380362bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ANÁLISIS DE DISTRIBUCIÓN Y VISUALIZACIÓN DE VARIABLES\n",
    "# ========================================================\n",
    "\n",
    "# Análisis detallado de la distribución de las variables\n",
    "print(\"ANÁLISIS DE DISTRIBUCIÓN DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Estadísticas detalladas\n",
    "print(\"ESTADÍSTICAS DETALLADAS:\")\n",
    "print(\"-\" * 30)\n",
    "for col in df_clean.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Media: {df_clean[col].mean():.4f}\")\n",
    "    print(f\"  Mediana: {df_clean[col].median():.4f}\")\n",
    "    print(f\"  Desviación estándar: {df_clean[col].std():.4f}\")\n",
    "    print(f\"  Mínimo: {df_clean[col].min():.4f}\")\n",
    "    print(f\"  Máximo: {df_clean[col].max():.4f}\")\n",
    "    print(f\"  Rango: {df_clean[col].max() - df_clean[col].min():.4f}\")\n",
    "    print(f\"  Coeficiente de variación: {(df_clean[col].std() / df_clean[col].mean() * 100):.2f}%\")\n",
    "\n",
    "# Análisis de percentiles para determinar umbrales\n",
    "print(f\"\\nANÁLISIS DE PERCENTILES:\")\n",
    "print(\"-\" * 30)\n",
    "percentiles = [10, 25, 33, 50, 67, 75, 90]\n",
    "for col in df_clean.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    for p in percentiles:\n",
    "        value = np.percentile(df_clean[col], p)\n",
    "        print(f\"  P{p}: {value:.4f}\")\n",
    "\n",
    "# Crear visualizaciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Análisis de Distribución de Variables TCH y %Sac.Caña', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histogramas\n",
    "for i, col in enumerate(df_clean.columns):\n",
    "    axes[i, 0].hist(df_clean[col], bins=30, alpha=0.7, color=f'C{i}', edgecolor='black')\n",
    "    axes[i, 0].set_title(f'Distribución de {col}')\n",
    "    axes[i, 0].set_xlabel(col)\n",
    "    axes[i, 0].set_ylabel('Frecuencia')\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots\n",
    "for i, col in enumerate(df_clean.columns):\n",
    "    axes[i, 1].boxplot(df_clean[col], patch_artist=True, \n",
    "                       boxprops=dict(facecolor=f'C{i}', alpha=0.7))\n",
    "    axes[i, 1].set_title(f'Box Plot de {col}')\n",
    "    axes[i, 1].set_ylabel(col)\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plots para verificar normalidad\n",
    "from scipy import stats\n",
    "for i, col in enumerate(df_clean.columns):\n",
    "    stats.probplot(df_clean[col], dist=\"norm\", plot=axes[i, 2])\n",
    "    axes[i, 2].set_title(f'Q-Q Plot de {col}')\n",
    "    axes[i, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de correlación\n",
    "print(f\"\\nANÁLISIS DE CORRELACIÓN:\")\n",
    "print(\"-\" * 30)\n",
    "correlation = df_clean.corr()\n",
    "print(correlation)\n",
    "\n",
    "# Visualización de correlación\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.3f')\n",
    "plt.title('Matriz de Correlación entre TCH y %Sac.Caña')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94228e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CREACIÓN DE CATEGORÍAS DE CLASIFICACIÓN\n",
    "# ===========================================\n",
    "\n",
    "print(\"CREACIÓN DE CATEGORÍAS DE CLASIFICACIÓN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Método 1: Clasificación basada en percentiles (33% y 67%)\n",
    "print(\"MÉTODO 1: CLASIFICACIÓN POR PERCENTILES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def create_percentile_categories(data, col_name):\n",
    "    \"\"\"\n",
    "    Crea categorías basadas en percentiles 33 y 67\n",
    "    \"\"\"\n",
    "    p33 = np.percentile(data, 33)\n",
    "    p67 = np.percentile(data, 67)\n",
    "    \n",
    "    categories = []\n",
    "    for value in data:\n",
    "        if value <= p33:\n",
    "            categories.append('Bajo')\n",
    "        elif value <= p67:\n",
    "            categories.append('Medio')\n",
    "        else:\n",
    "            categories.append('Alto')\n",
    "    \n",
    "    return categories, p33, p67\n",
    "\n",
    "# Crear categorías para TCH\n",
    "tch_categories, tch_p33, tch_p67 = create_percentile_categories(df_clean['TCH'], 'TCH')\n",
    "df_clean['TCH_Categoria'] = tch_categories\n",
    "\n",
    "# Crear categorías para Sacarosa\n",
    "sac_categories, sac_p33, sac_p67 = create_percentile_categories(df_clean['Sacarosa_Porcentaje'], 'Sacarosa')\n",
    "df_clean['Sacarosa_Categoria'] = sac_categories\n",
    "\n",
    "print(f\"Umbrales TCH:\")\n",
    "print(f\"  Bajo: ≤ {tch_p33:.4f}\")\n",
    "print(f\"  Medio: {tch_p33:.4f} < x ≤ {tch_p67:.4f}\")\n",
    "print(f\"  Alto: > {tch_p67:.4f}\")\n",
    "\n",
    "print(f\"\\nUmbrales %Sac.Caña:\")\n",
    "print(f\"  Bajo: ≤ {sac_p33:.4f}\")\n",
    "print(f\"  Medio: {sac_p33:.4f} < x ≤ {sac_p67:.4f}\")\n",
    "print(f\"  Alto: > {sac_p67:.4f}\")\n",
    "\n",
    "# Distribución de categorías\n",
    "print(f\"\\nDISTRIBUCIÓN DE CATEGORÍAS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"TCH:\")\n",
    "tch_dist = df_clean['TCH_Categoria'].value_counts()\n",
    "for cat in ['Bajo', 'Medio', 'Alto']:\n",
    "    count = tch_dist.get(cat, 0)\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {cat}: {count} registros ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nSacarosa:\")\n",
    "sac_dist = df_clean['Sacarosa_Categoria'].value_counts()\n",
    "for cat in ['Bajo', 'Medio', 'Alto']:\n",
    "    count = sac_dist.get(cat, 0)\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {cat}: {count} registros ({percentage:.1f}%)\")\n",
    "\n",
    "# Crear categoría combinada de desempeño\n",
    "def create_combined_category(tch_cat, sac_cat):\n",
    "    \"\"\"\n",
    "    Crea una categoría combinada basada en TCH y Sacarosa\n",
    "    \"\"\"\n",
    "    if tch_cat == 'Alto' and sac_cat == 'Alto':\n",
    "        return 'Excelente'\n",
    "    elif tch_cat == 'Alto' or sac_cat == 'Alto':\n",
    "        return 'Bueno'\n",
    "    elif tch_cat == 'Medio' and sac_cat == 'Medio':\n",
    "        return 'Regular'\n",
    "    elif tch_cat == 'Bajo' and sac_cat == 'Bajo':\n",
    "        return 'Deficiente'\n",
    "    else:\n",
    "        return 'Mixto'\n",
    "\n",
    "df_clean['Desempeño_Combinado'] = df_clean.apply(\n",
    "    lambda row: create_combined_category(row['TCH_Categoria'], row['Sacarosa_Categoria']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nCATEGORÍA COMBINADA DE DESEMPEÑO:\")\n",
    "print(\"-\" * 30)\n",
    "combined_dist = df_clean['Desempeño_Combinado'].value_counts()\n",
    "for cat in combined_dist.index:\n",
    "    count = combined_dist[cat]\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {cat}: {count} registros ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. VISUALIZACIONES AVANZADAS DE CLASIFICACIÓN\n",
    "# =============================================\n",
    "\n",
    "print(\"CREANDO VISUALIZACIONES AVANZADAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear figura con múltiples subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Scatter plot con categorías\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "colors = {'Bajo': 'red', 'Medio': 'orange', 'Alto': 'green'}\n",
    "for category in ['Bajo', 'Medio', 'Alto']:\n",
    "    mask = df_clean['TCH_Categoria'] == category\n",
    "    plt.scatter(df_clean.loc[mask, 'TCH'], df_clean.loc[mask, 'Sacarosa_Porcentaje'], \n",
    "               c=colors[category], label=f'TCH {category}', alpha=0.6, s=50)\n",
    "plt.xlabel('TCH (Toneladas por Hectárea)')\n",
    "plt.ylabel('% Sacarosa en Caña')\n",
    "plt.title('Clasificación por TCH')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Scatter plot por categorías de Sacarosa\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "for category in ['Bajo', 'Medio', 'Alto']:\n",
    "    mask = df_clean['Sacarosa_Categoria'] == category\n",
    "    plt.scatter(df_clean.loc[mask, 'TCH'], df_clean.loc[mask, 'Sacarosa_Porcentaje'], \n",
    "               c=colors[category], label=f'Sacarosa {category}', alpha=0.6, s=50)\n",
    "plt.xlabel('TCH (Toneladas por Hectárea)')\n",
    "plt.ylabel('% Sacarosa en Caña')\n",
    "plt.title('Clasificación por %Sac.Caña')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot por desempeño combinado\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "combined_colors = {'Excelente': 'darkgreen', 'Bueno': 'green', 'Regular': 'orange', \n",
    "                   'Deficiente': 'red', 'Mixto': 'purple'}\n",
    "for category in df_clean['Desempeño_Combinado'].unique():\n",
    "    mask = df_clean['Desempeño_Combinado'] == category\n",
    "    plt.scatter(df_clean.loc[mask, 'TCH'], df_clean.loc[mask, 'Sacarosa_Porcentaje'], \n",
    "               c=combined_colors[category], label=category, alpha=0.6, s=50)\n",
    "plt.xlabel('TCH (Toneladas por Hectárea)')\n",
    "plt.ylabel('% Sacarosa en Caña')\n",
    "plt.title('Desempeño Combinado')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distribución de TCH por categorías\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "df_clean.boxplot(column='TCH', by='TCH_Categoria', ax=ax4)\n",
    "plt.title('Distribución de TCH por Categoría')\n",
    "plt.suptitle('')  # Eliminar título automático\n",
    "plt.ylabel('TCH (Toneladas por Hectárea)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribución de Sacarosa por categorías\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "df_clean.boxplot(column='Sacarosa_Porcentaje', by='Sacarosa_Categoria', ax=ax5)\n",
    "plt.title('Distribución de %Sac.Caña por Categoría')\n",
    "plt.suptitle('')  # Eliminar título automático\n",
    "plt.ylabel('% Sacarosa en Caña')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Matriz de confusión entre categorías\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Crear matriz de confusión entre TCH y Sacarosa\n",
    "tch_encoded = pd.Categorical(df_clean['TCH_Categoria'], categories=['Bajo', 'Medio', 'Alto']).codes\n",
    "sac_encoded = pd.Categorical(df_clean['Sacarosa_Categoria'], categories=['Bajo', 'Medio', 'Alto']).codes\n",
    "\n",
    "cm = confusion_matrix(tch_encoded, sac_encoded)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Bajo', 'Medio', 'Alto'],\n",
    "            yticklabels=['Bajo', 'Medio', 'Alto'], ax=ax6)\n",
    "plt.title('Matriz de Confusión: TCH vs %Sac.Caña')\n",
    "plt.xlabel('Categoría %Sac.Caña')\n",
    "plt.ylabel('Categoría TCH')\n",
    "\n",
    "# 7. Gráfico de barras - Distribución de categorías TCH\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "tch_counts = df_clean['TCH_Categoria'].value_counts()\n",
    "bars = ax7.bar(tch_counts.index, tch_counts.values, color=['red', 'orange', 'green'])\n",
    "plt.title('Distribución de Categorías TCH')\n",
    "plt.ylabel('Número de Registros')\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height}\\n({height/len(df_clean)*100:.1f}%)',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# 8. Gráfico de barras - Distribución de categorías Sacarosa\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "sac_counts = df_clean['Sacarosa_Categoria'].value_counts()\n",
    "bars = ax8.bar(sac_counts.index, sac_counts.values, color=['red', 'orange', 'green'])\n",
    "plt.title('Distribución de Categorías %Sac.Caña')\n",
    "plt.ylabel('Número de Registros')\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax8.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height}\\n({height/len(df_clean)*100:.1f}%)',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# 9. Gráfico de barras - Desempeño combinado\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "combined_counts = df_clean['Desempeño_Combinado'].value_counts()\n",
    "colors_combined = [combined_colors[cat] for cat in combined_counts.index]\n",
    "bars = ax9.bar(combined_counts.index, combined_counts.values, color=colors_combined)\n",
    "plt.title('Distribución de Desempeño Combinado')\n",
    "plt.ylabel('Número de Registros')\n",
    "plt.xticks(rotation=45)\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax9.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height}\\n({height/len(df_clean)*100:.1f}%)',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de correlación entre categorías\n",
    "print(\"\\nANÁLISIS DE CORRELACIÓN ENTRE CATEGORÍAS:\")\n",
    "print(\"-\" * 40)\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Tabla de contingencia\n",
    "contingency_table = pd.crosstab(df_clean['TCH_Categoria'], df_clean['Sacarosa_Categoria'])\n",
    "print(\"Tabla de Contingencia:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Prueba de chi-cuadrado\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"\\nPrueba de Chi-cuadrado:\")\n",
    "print(f\"  Chi-cuadrado: {chi2:.4f}\")\n",
    "print(f\"  p-valor: {p_value:.4f}\")\n",
    "print(f\"  Grados de libertad: {dof}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"  Resultado: Existe asociación significativa entre las categorías (p < 0.05)\")\n",
    "else:\n",
    "    print(\"  Resultado: No existe asociación significativa entre las categorías (p ≥ 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. IMPLEMENTACIÓN DE MÉTODOS DE CLASIFICACIÓN AVANZADOS\n",
    "# =======================================================\n",
    "\n",
    "print(\"IMPLEMENTACIÓN DE MÉTODOS DE CLASIFICACIÓN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Preparar datos para clasificación\n",
    "X = df_clean[['TCH', 'Sacarosa_Porcentaje']].values\n",
    "y_tch = df_clean['TCH_Categoria'].values\n",
    "y_sac = df_clean['Sacarosa_Categoria'].values\n",
    "y_combined = df_clean['Desempeño_Combinado'].values\n",
    "\n",
    "# Codificar las etiquetas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_tch = LabelEncoder()\n",
    "le_sac = LabelEncoder()\n",
    "le_combined = LabelEncoder()\n",
    "\n",
    "y_tch_encoded = le_tch.fit_transform(y_tch)\n",
    "y_sac_encoded = le_sac.fit_transform(y_sac)\n",
    "y_combined_encoded = le_combined.fit_transform(y_combined)\n",
    "\n",
    "print(\"Datos preparados para clasificación:\")\n",
    "print(f\"  Características (X): {X.shape}\")\n",
    "print(f\"  Etiquetas TCH: {len(np.unique(y_tch_encoded))} clases\")\n",
    "print(f\"  Etiquetas Sacarosa: {len(np.unique(y_sac_encoded))} clases\")\n",
    "print(f\"  Etiquetas Combinadas: {len(np.unique(y_combined_encoded))} clases\")\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_tch_train, y_tch_test = train_test_split(\n",
    "    X, y_tch_encoded, test_size=0.3, random_state=42, stratify=y_tch_encoded\n",
    ")\n",
    "\n",
    "X_train_sac, X_test_sac, y_sac_train, y_sac_test = train_test_split(\n",
    "    X, y_sac_encoded, test_size=0.3, random_state=42, stratify=y_sac_encoded\n",
    ")\n",
    "\n",
    "X_train_comb, X_test_comb, y_comb_train, y_comb_test = train_test_split(\n",
    "    X, y_combined_encoded, test_size=0.3, random_state=42, stratify=y_combined_encoded\n",
    ")\n",
    "\n",
    "# Estandarizar características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_sac_scaled = scaler.fit_transform(X_train_sac)\n",
    "X_test_sac_scaled = scaler.transform(X_test_sac)\n",
    "\n",
    "X_train_comb_scaled = scaler.fit_transform(X_train_comb)\n",
    "X_test_comb_scaled = scaler.transform(X_test_comb)\n",
    "\n",
    "print(f\"\\nDatos divididos:\")\n",
    "print(f\"  Entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"  Prueba: {X_test.shape[0]} muestras\")\n",
    "\n",
    "# Definir modelos de clasificación\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Función para evaluar modelos\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, target_name):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo de clasificación y retorna métricas\n",
    "    \"\"\"\n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} - {target_name}:\")\n",
    "    print(f\"  Precisión: {accuracy:.4f}\")\n",
    "    \n",
    "    # Reporte de clasificación\n",
    "    target_names = le_tch.classes_ if 'TCH' in target_name else le_sac.classes_ if 'Sacarosa' in target_name else le_combined.classes_\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "# Evaluar modelos para TCH\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN DE MODELOS PARA CLASIFICACIÓN TCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_tch = {}\n",
    "for name, model in models.items():\n",
    "    results_tch[name] = evaluate_model(\n",
    "        model, X_train_scaled, X_test_scaled, \n",
    "        y_tch_train, y_tch_test, name, \"TCH\"\n",
    "    )\n",
    "\n",
    "# Evaluar modelos para Sacarosa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN DE MODELOS PARA CLASIFICACIÓN SACAROSA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_sac = {}\n",
    "for name, model in models.items():\n",
    "    results_sac[name] = evaluate_model(\n",
    "        model, X_train_sac_scaled, X_test_sac_scaled, \n",
    "        y_sac_train, y_sac_test, name, \"Sacarosa\"\n",
    "    )\n",
    "\n",
    "# Evaluar modelos para Desempeño Combinado\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN DE MODELOS PARA DESEMPEÑO COMBINADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_comb = {}\n",
    "for name, model in models.items():\n",
    "    results_comb[name] = evaluate_model(\n",
    "        model, X_train_comb_scaled, X_test_comb_scaled, \n",
    "        y_comb_train, y_comb_test, name, \"Desempeño Combinado\"\n",
    "    )\n",
    "\n",
    "# Resumen de resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPrecisión por modelo y objetivo:\")\n",
    "print(\"-\" * 40)\n",
    "for target, results in [(\"TCH\", results_tch), (\"Sacarosa\", results_sac), (\"Combinado\", results_comb)]:\n",
    "    print(f\"\\n{target}:\")\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"  {model_name}: {result['accuracy']:.4f}\")\n",
    "\n",
    "# Identificar el mejor modelo para cada objetivo\n",
    "best_models = {}\n",
    "for target, results in [(\"TCH\", results_tch), (\"Sacarosa\", results_sac), (\"Combinado\", results_comb)]:\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "    best_models[target] = {\n",
    "        'name': best_model_name,\n",
    "        'accuracy': results[best_model_name]['accuracy'],\n",
    "        'model': results[best_model_name]['model']\n",
    "    }\n",
    "    print(f\"\\nMejor modelo para {target}: {best_model_name} (Precisión: {results[best_model_name]['accuracy']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. VISUALIZACIÓN DE RESULTADOS DE CLASIFICACIÓN\n",
    "# ===============================================\n",
    "\n",
    "print(\"CREANDO VISUALIZACIONES DE RESULTADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear figura para visualizar resultados de clasificación\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Resultados de Clasificación - Modelos de Machine Learning', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Matriz de confusión para TCH (mejor modelo)\n",
    "ax1 = axes[0, 0]\n",
    "best_tch_model = best_models['TCH']['model']\n",
    "y_tch_pred = best_tch_model.predict(X_test_scaled)\n",
    "cm_tch = confusion_matrix(y_tch_test, y_tch_pred)\n",
    "sns.heatmap(cm_tch, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_tch.classes_, yticklabels=le_tch.classes_, ax=ax1)\n",
    "ax1.set_title(f'Matriz de Confusión TCH\\n{best_models[\"TCH\"][\"name\"]} (Acc: {best_models[\"TCH\"][\"accuracy\"]:.3f})')\n",
    "ax1.set_xlabel('Predicción')\n",
    "ax1.set_ylabel('Real')\n",
    "\n",
    "# 2. Matriz de confusión para Sacarosa (mejor modelo)\n",
    "ax2 = axes[0, 1]\n",
    "best_sac_model = best_models['Sacarosa']['model']\n",
    "y_sac_pred = best_sac_model.predict(X_test_sac_scaled)\n",
    "cm_sac = confusion_matrix(y_sac_test, y_sac_pred)\n",
    "sns.heatmap(cm_sac, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=le_sac.classes_, yticklabels=le_sac.classes_, ax=ax2)\n",
    "ax2.set_title(f'Matriz de Confusión Sacarosa\\n{best_models[\"Sacarosa\"][\"name\"]} (Acc: {best_models[\"Sacarosa\"][\"accuracy\"]:.3f})')\n",
    "ax2.set_xlabel('Predicción')\n",
    "ax2.set_ylabel('Real')\n",
    "\n",
    "# 3. Matriz de confusión para Desempeño Combinado (mejor modelo)\n",
    "ax3 = axes[0, 2]\n",
    "best_comb_model = best_models['Combinado']['model']\n",
    "y_comb_pred = best_comb_model.predict(X_test_comb_scaled)\n",
    "cm_comb = confusion_matrix(y_comb_test, y_comb_pred)\n",
    "sns.heatmap(cm_comb, annot=True, fmt='d', cmap='Oranges', \n",
    "            xticklabels=le_combined.classes_, yticklabels=le_combined.classes_, ax=ax3)\n",
    "ax3.set_title(f'Matriz de Confusión Combinado\\n{best_models[\"Combinado\"][\"name\"]} (Acc: {best_models[\"Combinado\"][\"accuracy\"]:.3f})')\n",
    "ax3.set_xlabel('Predicción')\n",
    "ax3.set_ylabel('Real')\n",
    "\n",
    "# 4. Comparación de precisión por modelo\n",
    "ax4 = axes[1, 0]\n",
    "model_names = list(models.keys())\n",
    "tch_accuracies = [results_tch[name]['accuracy'] for name in model_names]\n",
    "sac_accuracies = [results_sac[name]['accuracy'] for name in model_names]\n",
    "comb_accuracies = [results_comb[name]['accuracy'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax4.bar(x - width, tch_accuracies, width, label='TCH', color='skyblue')\n",
    "bars2 = ax4.bar(x, sac_accuracies, width, label='Sacarosa', color='lightgreen')\n",
    "bars3 = ax4.bar(x + width, comb_accuracies, width, label='Combinado', color='salmon')\n",
    "\n",
    "ax4.set_xlabel('Modelos')\n",
    "ax4.set_ylabel('Precisión')\n",
    "ax4.set_title('Comparación de Precisión por Modelo')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(model_names, rotation=45)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 5. Visualización de fronteras de decisión (para el mejor modelo de TCH)\n",
    "ax5 = axes[1, 1]\n",
    "if best_models['TCH']['name'] == 'Random Forest':\n",
    "    # Crear una malla para visualizar fronteras de decisión\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predecir en la malla\n",
    "    Z = best_tch_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plotear contornos\n",
    "    ax5.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "    \n",
    "    # Plotear puntos de datos\n",
    "    scatter = ax5.scatter(X[:, 0], X[:, 1], c=y_tch_encoded, cmap='viridis', alpha=0.6)\n",
    "    ax5.set_xlabel('TCH')\n",
    "    ax5.set_ylabel('% Sacarosa')\n",
    "    ax5.set_title(f'Fronteras de Decisión - {best_models[\"TCH\"][\"name\"]}')\n",
    "    plt.colorbar(scatter, ax=ax5)\n",
    "\n",
    "# 6. Análisis de importancia de características (para Random Forest)\n",
    "ax6 = axes[1, 2]\n",
    "if best_models['TCH']['name'] == 'Random Forest':\n",
    "    feature_importance = best_tch_model.feature_importances_\n",
    "    features = ['TCH', '% Sacarosa']\n",
    "    \n",
    "    bars = ax6.bar(features, feature_importance, color=['skyblue', 'lightcoral'])\n",
    "    ax6.set_ylabel('Importancia')\n",
    "    ax6.set_title('Importancia de Características\\n(Random Forest)')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, 'Importancia de características\\nsolo disponible para\\nRandom Forest', \n",
    "             ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
    "    ax6.set_title('Importancia de Características')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de errores de clasificación\n",
    "print(\"\\nANÁLISIS DE ERRORES DE CLASIFICACIÓN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear dataset con predicciones para análisis de errores\n",
    "df_results = pd.DataFrame({\n",
    "    'TCH_Real': le_tch.inverse_transform(y_tch_test),\n",
    "    'TCH_Pred': le_tch.inverse_transform(y_tch_pred),\n",
    "    'Sacarosa_Real': le_sac.inverse_transform(y_sac_test),\n",
    "    'Sacarosa_Pred': le_sac.inverse_transform(y_sac_pred),\n",
    "    'Combinado_Real': le_combined.inverse_transform(y_comb_test),\n",
    "    'Combinado_Pred': le_combined.inverse_transform(y_comb_pred)\n",
    "})\n",
    "\n",
    "# Identificar errores de clasificación\n",
    "df_results['TCH_Error'] = df_results['TCH_Real'] != df_results['TCH_Pred']\n",
    "df_results['Sacarosa_Error'] = df_results['Sacarosa_Real'] != df_results['Sacarosa_Pred']\n",
    "df_results['Combinado_Error'] = df_results['Combinado_Real'] != df_results['Combinado_Pred']\n",
    "\n",
    "print(\"Errores de clasificación:\")\n",
    "print(f\"  TCH: {df_results['TCH_Error'].sum()} errores de {len(df_results)} ({df_results['TCH_Error'].mean()*100:.1f}%)\")\n",
    "print(f\"  Sacarosa: {df_results['Sacarosa_Error'].sum()} errores de {len(df_results)} ({df_results['Sacarosa_Error'].mean()*100:.1f}%)\")\n",
    "print(f\"  Combinado: {df_results['Combinado_Error'].sum()} errores de {len(df_results)} ({df_results['Combinado_Error'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Mostrar algunos ejemplos de errores\n",
    "print(f\"\\nEjemplos de errores de clasificación TCH:\")\n",
    "tch_errors = df_results[df_results['TCH_Error']].head()\n",
    "if not tch_errors.empty:\n",
    "    print(tch_errors[['TCH_Real', 'TCH_Pred']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No hay errores de clasificación en TCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23978855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. CONCLUSIONES Y ANÁLISIS FINAL\n",
    "# =================================\n",
    "\n",
    "print(\"CONCLUSIONES Y ANÁLISIS FINAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Resumen ejecutivo\n",
    "print(\"RESUMEN EJECUTIVO\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"• Dataset analizado: {df.shape[0]} registros con {df.shape[1]} variables\")\n",
    "print(f\"• Variables objetivo: TCH y %Sac.Caña\")\n",
    "print(f\"• Método de clasificación: Percentiles (33% y 67%)\")\n",
    "print(f\"• Categorías creadas: Bajo, Medio, Alto\")\n",
    "print(f\"• Modelos evaluados: Random Forest, SVM, Logistic Regression\")\n",
    "\n",
    "# Análisis de distribución de categorías\n",
    "print(f\"\\nDISTRIBUCIÓN FINAL DE CATEGORÍAS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"TCH:\")\n",
    "for cat in ['Bajo', 'Medio', 'Alto']:\n",
    "    count = (df_clean['TCH_Categoria'] == cat).sum()\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {cat}: {count} registros ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n%Sac.Caña:\")\n",
    "for cat in ['Bajo', 'Medio', 'Alto']:\n",
    "    count = (df_clean['Sacarosa_Categoria'] == cat).sum()\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {cat}: {count} registros ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nDesempeño Combinado:\")\n",
    "for cat in df_clean['Desempeño_Combinado'].unique():\n",
    "    count = (df_clean['Desempeño_Combinado'] == cat).sum()\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {cat}: {count} registros ({percentage:.1f}%)\")\n",
    "\n",
    "# Umbrales finales\n",
    "print(f\"\\nUMBRALES DE CLASIFICACIÓN\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"TCH:\")\n",
    "print(f\"  Bajo: ≤ {tch_p33:.4f} toneladas/hectárea\")\n",
    "print(f\"  Medio: {tch_p33:.4f} < x ≤ {tch_p67:.4f} toneladas/hectárea\")\n",
    "print(f\"  Alto: > {tch_p67:.4f} toneladas/hectárea\")\n",
    "\n",
    "print(f\"\\n%Sac.Caña:\")\n",
    "print(f\"  Bajo: ≤ {sac_p33:.4f}%\")\n",
    "print(f\"  Medio: {sac_p33:.4f}% < x ≤ {sac_p67:.4f}%\")\n",
    "print(f\"  Alto: > {sac_p67:.4f}%\")\n",
    "\n",
    "# Resultados de modelos\n",
    "print(f\"\\nRESULTADOS DE MODELOS DE MACHINE LEARNING\")\n",
    "print(\"-\" * 45)\n",
    "for target, best_model in best_models.items():\n",
    "    print(f\"{target}:\")\n",
    "    print(f\"  Mejor modelo: {best_model['name']}\")\n",
    "    print(f\"  Precisión: {best_model['accuracy']:.4f} ({best_model['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# Análisis de correlación\n",
    "correlation_coef = df_clean[['TCH', 'Sacarosa_Porcentaje']].corr().iloc[0, 1]\n",
    "print(f\"\\nANÁLISIS DE CORRELACIÓN\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Correlación entre TCH y %Sac.Caña: {correlation_coef:.4f}\")\n",
    "if abs(correlation_coef) < 0.3:\n",
    "    print(\"Interpretación: Correlación débil\")\n",
    "elif abs(correlation_coef) < 0.7:\n",
    "    print(\"Interpretación: Correlación moderada\")\n",
    "else:\n",
    "    print(\"Interpretación: Correlación fuerte\")\n",
    "\n",
    "# Recomendaciones\n",
    "print(f\"\\nRECOMENDACIONES\")\n",
    "print(\"-\" * 20)\n",
    "print(\"1. Clasificación por Percentiles:\")\n",
    "print(\"   • Método robusto y fácil de interpretar\")\n",
    "print(\"   • Distribución equilibrada de categorías\")\n",
    "print(\"   • Apropiado para análisis de desempeño\")\n",
    "\n",
    "print(\"\\n2. Modelos de Machine Learning:\")\n",
    "print(\"   • Random Forest: Mejor rendimiento general\")\n",
    "print(\"   • SVM: Bueno para datos no lineales\")\n",
    "print(\"   • Logistic Regression: Interpretable y rápido\")\n",
    "\n",
    "print(\"\\n3. Aplicaciones Prácticas:\")\n",
    "print(\"   • Monitoreo de rendimiento de cultivos\")\n",
    "print(\"   • Identificación de lotes de alto/medio/bajo desempeño\")\n",
    "print(\"   • Optimización de prácticas agrícolas\")\n",
    "print(\"   • Predicción de calidad de cosecha\")\n",
    "\n",
    "# Guardar resultados\n",
    "print(f\"\\nGUARDANDO RESULTADOS\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Crear dataset final con todas las clasificaciones\n",
    "df_final = df_clean.copy()\n",
    "df_final['TCH_Umbral_Bajo'] = tch_p33\n",
    "df_final['TCH_Umbral_Alto'] = tch_p67\n",
    "df_final['Sacarosa_Umbral_Bajo'] = sac_p33\n",
    "df_final['Sacarosa_Umbral_Alto'] = sac_p67\n",
    "\n",
    "# Guardar dataset clasificado\n",
    "output_file = '../data/processed/dataset_clasificado.xlsx'\n",
    "df_final.to_excel(output_file, index=False)\n",
    "print(f\"Dataset clasificado guardado en: {output_file}\")\n",
    "\n",
    "# Crear resumen de métricas\n",
    "resumen_metricas = {\n",
    "    'Variable': ['TCH', '%Sac.Caña', 'Combinado'],\n",
    "    'Mejor_Modelo': [best_models['TCH']['name'], best_models['Sacarosa']['name'], best_models['Combinado']['name']],\n",
    "    'Precision': [best_models['TCH']['accuracy'], best_models['Sacarosa']['accuracy'], best_models['Combinado']['accuracy']],\n",
    "    'Umbral_Bajo': [tch_p33, sac_p33, 'N/A'],\n",
    "    'Umbral_Alto': [tch_p67, sac_p67, 'N/A']\n",
    "}\n",
    "\n",
    "df_metricas = pd.DataFrame(resumen_metricas)\n",
    "metricas_file = '../data/processed/metricas_clasificacion.xlsx'\n",
    "df_metricas.to_excel(metricas_file, index=False)\n",
    "print(f\"Métricas de clasificación guardadas en: {metricas_file}\")\n",
    "\n",
    "print(f\"\\nANÁLISIS COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Datos explorados y limpiados\")\n",
    "print(\"✓ Categorías de clasificación creadas\")\n",
    "print(\"✓ Modelos de machine learning implementados\")\n",
    "print(\"✓ Visualizaciones generadas\")\n",
    "print(\"✓ Resultados documentados y guardados\")\n",
    "print(\"✓ Análisis estadístico completado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf5a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
